{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2707c6bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cd9a4b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "from train import train\n",
    "from build_model import build_model\n",
    "from data import build_synthetic_dataset\n",
    "from model import ARModel\n",
    "from utils import Logger\n",
    "\n",
    "import traceback\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "14b93ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "global_config = dict(\n",
    "    ver=\"mar_init\",\n",
    "    description=\"init\",\n",
    "    outcome_root=\"test\",\n",
    ")\n",
    "global_config[\"outcome_dir_root\"] = os.path.join(global_config[\"outcome_root\"],\n",
    "                                                 global_config[\"ver\"])\n",
    "\n",
    "transformer_config=dict(\n",
    "    inp_dim = 16,\n",
    "    dim = 32,\n",
    "    out_dim = 16,\n",
    "    num_layers = 4,\n",
    "    num_heads = 8,\n",
    "    ff_hidden_dim = 64,\n",
    "    max_seq_len = 64,\n",
    "    dropout = 0.0\n",
    ")\n",
    "\n",
    "\n",
    "mlp_config=dict(\n",
    "    in_channels=transformer_config['inp_dim'],\n",
    "    model_channels=32,\n",
    "    out_channels=transformer_config['inp_dim'],\n",
    "    z_channels=transformer_config['out_dim'],\n",
    "    num_res_blocks=2,\n",
    "    grad_checkpointing=False\n",
    ")\n",
    "\n",
    "\n",
    "diffusion_config=dict(\n",
    "    num_steps = 64\n",
    ")\n",
    "\n",
    "train_config=dict(\n",
    "    num_fm_per_gd=4,\n",
    "    max_seq_len=transformer_config['max_seq_len'],\n",
    "    train_steps=12,\n",
    "    log_every_n_steps=3,\n",
    "    eval_every_n_steps=3,\n",
    "    pretrained=None,\n",
    "    batch_size=32,\n",
    "    base_learning_rate=5.0e-5,\n",
    "    min_learning_rate=4.0e-5,\n",
    "    use_lr_scheduler=True,\n",
    "    warmup_steps=3,\n",
    "    betas=[0.98, 0.999],\n",
    "    need_check=False,\n",
    "    use_ema=False,\n",
    "    ema_decay=0.9999,\n",
    "    ema_steps=20000\n",
    ")\n",
    "train_config['save']=train_config['train_steps']>0\n",
    "\n",
    "dataset_paths={'afhq':'/kaggle/input/afhq-512',\n",
    "               'ffhq':'/kaggle/input/flickrfaceshq-dataset-nvidia-resized-256px',\n",
    "               'celebahq':'/kaggle/input/celebahq256-images-only',\n",
    "               'fa':'/kaggle/input/face-attributes-grouped',\n",
    "               'animestyle':'/kaggle/input/gananime-lite',\n",
    "               'animefaces':'/kaggle/input/another-anime-face-dataset',\n",
    "              }\n",
    "\n",
    "data_config = dict(\n",
    "    shape=(train_config['batch_size'],\n",
    "           train_config['max_seq_len'],\n",
    "           transformer_config['inp_dim']),\n",
    "    image_size=256,\n",
    "    batch_size=train_config['batch_size'],\n",
    "    ae_batch_size=48,\n",
    "    split=[0.5,0.25,0.25],\n",
    "    data_paths=dataset_paths,\n",
    "    enc_path=os.path.join(global_config[\"outcome_dir_root\"], \"enc\"),\n",
    "    enc_inp_path='/kaggle/input/sd-vae-ft-ema-f8-256-faces6-enc',\n",
    "    dataset_names=['afhq', 'ffhq', 'celebahq', 'fa', 'animestyle', 'animefaces'],\n",
    "    ignored_dataset=['fa'],\n",
    "    ignored_dataset_ft=['ffhq', 'celebahq', 'animestyle', 'animefaces'],\n",
    "    valid_dataset_idx=[]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "40f69d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = Logger(log_every_n_steps=train_config['log_every_n_steps'],\n",
    "                log_root=global_config[\"outcome_dir_root\"],\n",
    "                model_name=global_config['ver']\n",
    "               )\n",
    "\n",
    "logger.log_text(str(global_config), \"config\")\n",
    "logger.log_text(str(mlp_config), \"config\", newline=True)\n",
    "logger.log_text(str(transformer_config), \"config\", newline=True)\n",
    "logger.log_text(str(diffusion_config), \"config\", newline=True)\n",
    "logger.log_text(str(train_config), \"config\", newline=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "53a62ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42+hash(global_config['ver'])%10000)\n",
    "\n",
    "train_dataset, val_dataset, test_dataset = build_synthetic_dataset(data_config)\n",
    "\n",
    "logger.log_text(str(data_config), \"config\", newline=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "84d56590",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T params: 42,416, MLP params: 23,696, TTrainable: 42,416\n",
      "running on cuda\n"
     ]
    }
   ],
   "source": [
    "model, optim, lr_scheduler = build_model(logger,\n",
    "                                         transformer_config,\n",
    "                                         mlp_config,\n",
    "                                         diffusion_config,\n",
    "                                         train_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "66368c32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train step 3\n",
      "loss: 2.0023\n",
      "time per kstep: 1124\n",
      "peak GPU mem: 0.1 GB\n",
      "\n",
      "Test\n",
      "loss:2.0002+-0.0102\n",
      "Train step 6\n",
      "loss: 1.9948\n",
      "time per kstep: 2974\n",
      "peak GPU mem: 0.1 GB\n",
      "\n",
      "Test\n",
      "loss:2.0005+-0.0106\n",
      "Train step 9\n",
      "loss: 2.0076\n",
      "time per kstep: 2962\n",
      "peak GPU mem: 0.1 GB\n",
      "\n",
      "Test\n",
      "loss:2.0003+-0.0104\n",
      "Train step 12\n",
      "loss: 1.9943\n",
      "time per kstep: 2963\n",
      "peak GPU mem: 0.1 GB\n",
      "\n",
      "Test\n",
      "loss:2.0004+-0.0102\n",
      "Test\n",
      "loss:2.0000+-0.0103\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    train(model, optim, lr_scheduler, train_config,\n",
    "          train_dataset, val_dataset, test_dataset, logger)\n",
    "except Exception as e:\n",
    "    traceback.print_exc()\n",
    "    info = traceback.format_exc()\n",
    "    info = f\"Exception: {str(info)} \\n\"+\\\n",
    "            f\"Step: {logger.step}\"\n",
    "    print(info)\n",
    "    logger.log_text(info, \"error\")\n",
    "finally:\n",
    "    if not any([fn.endswith('.pth') for fn in os.listdir(logger.log_root)]):\n",
    "        if train_config['save']:\n",
    "            logger.log_net(model.cpu(),f\"mar_{logger.step}\")\n",
    "    shutil.make_archive(global_config[\"outcome_dir_root\"],\n",
    "                        'zip',\n",
    "                        global_config[\"outcome_dir_root\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "334b2b7e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
